# -*- coding: utf-8 -*-
"""customer_churn_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e-kmK3sC6jK_cAfSP_aQT6-uKcYp-whz

#**Customer Churn Prediction**

**Importing Libraries**
"""

# Import necessary libraries

import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

"""**Load the Dataset**"""

# Load and read the dataset

df=pd.read_csv('/content/telecom_customer_churn[1].csv')
df

"""**Overviewing all the rows and columns in the Dataset**"""

# Headings

df.head(5)

# Columns

df.columns

# Copy

df1 = df.copy()
df1

# Headings of copy

df1.head(7)

"""**Exploratory Data Analysis**

**Data Preprocessing**

**Dropping unwanted columns from the dataset**
"""

df1.drop(['Customer ID','Total Refunds','Zip Code','Latitude', 'Longitude','Churn Category', 'Churn Reason'],axis='columns',inplace=True)

# Shaping

df1.shape

# dtypes

df1.dtypes

"""**Checking the number of unique values in each column**"""

# Iterate through DataFrame columns and print unique value counts

for feature in df.columns:
    unique_values = df[feature].nunique()
    print(f'{feature} ---> {unique_values}')

"""**Getting the percentge of Null Values in each Column**"""

# Calculate percentage of missing values for each column

missing_percentage = df1.isnull().sum() / df1.shape[0]

# Display the result

print(missing_percentage)

"""**Cleaning Function for the Dataset**"""

# Cleaning Function

def clean_dataset(df):
    assert isinstance(df, pd.DataFrame)
    df.dropna(inplace=True)
    indices_to_keep = df.isin([np.nan, np.inf, -np.inf]).any(1)
    return df[indices_to_keep].astype(np.float64)

# Interpolate

df1=df1.interpolate()
df1

# Removing null values

df1=df1.dropna()
df.head()

# Extracting the data

df['Unlimited Data']

# Initializing the number_columns

number_columns=['Age','Number of Dependents','Number of Referrals','Tenure in Months','Avg Monthly Long Distance Charges','Avg Monthly GB Download','Monthly Charge','Total Charges','Total Extra Data Charges','Total Long Distance Charges','Total Revenue']
number_columns

"""**Checking the unique values of column having datatype: 'object'**"""

# Unique values function

def unique_values_names(df):
    for column in df:
        if df[column].dtype=='object':
            print(f'{column}:{df[column].unique()}')
unique_values_names(df1)

"""**Data Visualization**

**Visualizing Column 'Age' in the dataset**
"""

# Histogram

fig = px.histogram(df1, x = 'Age')
fig.show()

"""**Checking the stats in number_columns of the copied dataset**"""

# Different Histograms

df1.hist(figsize=(15,15), xrot=30)

# Extracting Age

df1['Age']

"""**Visualizing the number of customers who churned, stayed or joined in the company with a bar plot**"""

# Bar Plots for Customer_Stayed, Customer_Churned, Customer_Joined

Customer_Stayed=df1[df1['Customer Status']=='Stayed'].Age
Customer_Churned=df1[df1['Customer Status']=='Churned'].Age
Customer_Joined=df1[df1['Customer Status']=='Joined'].Age

plt.xlabel('Age')
plt.ylabel('Customers Numbers')
plt.hist([Customer_Stayed,Customer_Churned,Customer_Joined], color=['black','red','blue'],label=['Stayed','Churned','Joined'])

plt.title('Customers Behavior ',fontweight ="bold")
plt.legend()

"""**Defining Correlation between the columns in the dataset**"""

#Heatmap

data  = df1.corr()
plt.figure(figsize = (20,10))
sns.heatmap(data, annot = True)

"""**Analyzing Outlier in the dataset with respect to customer status**"""

# Boxplot

fig, ax = plt.subplots(4,3, figsize = (15,15))
for i, subplot in zip(number_columns, ax.flatten()):
    sns.boxplot(x = 'Customer Status', y = i , data = df1, ax = subplot)

# Heatmap

fig = px.density_heatmap(df1, x='Age', y='Total Charges')
fig.show()

# Extracting Columns

df1.columns

# Bar Plot for Married

pd.crosstab(df['Customer Status'], df['Married']).plot(kind='bar')

# Bar Plot for Gender

pd.crosstab(df['Customer Status'], df['Gender']).plot(kind='bar')

# Extracting Unique Values

df1['Payment Method'].unique()

"""**Create dictionary with role / data key value pairs**"""

Roles = {}
for j in df1['Payment Method'].unique():
    Roles[j] = df1[df1['Payment Method'] == j]

Roles.keys()

"""**Selecting the rows where the role is 'Credit Card'**"""

Roles['Credit Card']

# Length of Roles

len(Roles)

"""**Checking the number of Offers in the dataset**"""

# Counting Offer

off = df1['Offer'].value_counts()
off

# Bar Plot

fig = go.Figure([go.Bar(x=off.index, y=off.values)])
fig.show()

# Counting Credit Card in Roles

df1_off = Roles['Credit Card'].Offer.value_counts()
df1_off

# Bar Plot

fig = go.Figure([go.Bar(x= df1_off.index, y=df1_off.values)])
fig.show()

# Renaming

df1 = df1.rename(columns = {'Customer Status':'Customer_Status'})
df1

Roles1 = {}
for k in df1['Customer_Status'].unique():
    Roles1[k] = df1[df1['Customer_Status'] == k]
Roles1.keys()

df1_state = Roles1['Stayed'].Offer.value_counts()
df1_state

"""**Data Modelling**

**Replacing the Gender column in the dataset with Label Encoding**

0 for Female

1 for Male
"""

# Replace

df1.replace({"Gender":{'Female':0,'Male':1}},inplace=True)

"""**Replacing the columns with 'yes' and 'no' output by Label Encoding**

0 for No

1 for Yes  
"""

yes_and_no=[  'Paperless Billing', 'Unlimited Data',
       'Streaming Movies', 'Streaming Music',  'Streaming TV',
       'Premium Tech Support', 'Device Protection Plan', 'Online Backup', 'Online Security',
       'Multiple Lines',  'Married']
for i in yes_and_no:
    df1.replace({'No':0,'Yes':1},inplace=True)

"""**Replacing 'Phone Service' with '1'**"""

df1.replace({"Phone Service":{'Yes':1}},inplace=True)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df1.Customer_Status = le.fit_transform(df1.Customer_Status)

df1 = pd.get_dummies(data=df1, columns=['Payment Method','Contract','Internet Type','Offer','City'])

cols_to_scale = ['Age','Number of Dependents','Number of Referrals','Tenure in Months','Avg Monthly Long Distance Charges','Avg Monthly GB Download','Monthly Charge', 'Total Charges',
       'Total Extra Data Charges', 'Total Long Distance Charges','Total Revenue']

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df1[cols_to_scale] = scaler.fit_transform(df1[cols_to_scale])

"""**Dealing with Imbalance Data**

**Dropping the Customer_Status**

**i.e. The column tht we have to predict and set as a dependent variable bold text**
"""

# Dropping

X = df1.drop('Customer_Status',axis='columns')
y = df1['Customer_Status']

# Headings

X.head(5)

# Headings

y.head(5)

"""**Data Model Building**

**Spliting the data in Training and Test Data**
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=5)

# Length

len(X_train)

# Indexing

X_train[:10]

"""**Importing the required files for the model that is to applied**

1. Random Forest Classifier

2. Logistic Regression

3. GaussianNB

4. Decision Tree Classifier

5. XGB Classifier

**Importing Models**

**Getting the best_score from the applied models**
"""

scores = []
cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'], mp['params'], cv=cv, return_train_score=False)
    clf.fit(X,y)
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })

df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df

"""It was concluded that XGB_Classifier was giving us the best_score in the dataset

**Selecting the model with best score for the dataset**
"""

reg=XGBClassifier()
reg.fit(X_train, y_train)

reg.score(X_test, y_test)

"""We got an accuracy of 81.38 percent in the testing dataset

**Predicting values from the model build to check the accuracy**
"""

y_predicted = reg.predict(X_test)
y_predicted[:5]

"""**Verifying the actual values with the predicted values**"""

y_test[:5]

"""**Importing Confusion Matrix**"""

cm = confusion_matrix(y_test, y_predicted)
plt.figure(figsize = (10,7))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

"""**Importing Classification Report**"""

print(classification_report(y_test, y_predicted))

accuracy_score(y_test, y_predicted)

"""**Conclusion:**

In the end we conclude that the Telecom Customer Churn Prediction was best worked with XGB_Classifier with an accuracy score of 81.38%
"""